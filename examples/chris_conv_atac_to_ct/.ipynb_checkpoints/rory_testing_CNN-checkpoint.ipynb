{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../../bin')\n",
    "from helpers import Load_Dataset\n",
    "from RNA_functions import celltype_function\n",
    "from trainer import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-23 23:43:09 INFO     Cuda available: True\n",
      "2021-10-23 23:43:09 INFO     Cuda device count: 4\n",
      "2021-10-23 23:43:09 INFO     Cuda device name: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# Log GPU status\n",
    "is_cuda = torch.cuda.is_available()\n",
    "logging.info(\"Cuda available: \" + str(is_cuda))\n",
    "if is_cuda:\n",
    "    current_device = torch.cuda.current_device()\n",
    "    #torch.cuda.device(current_device)\n",
    "    device_count = torch.cuda.device_count()\n",
    "    logging.info(\"Cuda device count: \" + str(device_count))\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    logging.info(\"Cuda device name: \" + str(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celltype_function(RNA_adata):\n",
    "    \"\"\"\n",
    "    this code is taken from Chris, just to demonstrate wrapping into a function\n",
    "    \"\"\"\n",
    "    # Find cell types and get index labels\n",
    "    ct_grouped = RNA_adata.obs.groupby(\"cell_type\").size()\n",
    "    df_ct_grouped = pd.DataFrame(ct_grouped, columns=[\"count\"])\n",
    "    df_ct_grouped = df_ct_grouped.reset_index()\n",
    "    df_ct_grouped['label_id'] = df_ct_grouped.index\n",
    "\n",
    "    # Merge label ids with obs\n",
    "    RNA_adata.obs = RNA_adata.obs.reset_index().merge(df_ct_grouped, on='cell_type', how='inner').set_index('index')\n",
    "    return np.array(RNA_adata.obs.label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_path = '/camp/lab/briscoej/working/Rory/transcriptomics/NeurIPS_2021/nips_2021/multiome/multiome_atac_processed_training.h5ad'\n",
    "rna_path = '/camp/lab/briscoej/working/Rory/transcriptomics/NeurIPS_2021/nips_2021/multiome/multiome_gex_processed_training.h5ad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, dataset):\n",
    "#         super().__init__()\n",
    "#         self.embed1 = nn.Linear(dataset.n_peaks, 100)\n",
    "#         self.fc1 = nn.Linear(100, 200)\n",
    "#         self.fc2 = nn.Linear(200, 100)\n",
    "#         self.fc3 = nn.Linear(100, dataset.n_labels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.embed1(x))\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    basic feed forward model to test things out...\n",
    "    \"\"\"\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__()\n",
    "        # could just do two 1d convs to a feed forward network, keep simple...\n",
    "        \n",
    "        self.embed = nn.Linear(dataset.n_peaks,5000)\n",
    "        self.conv1a = nn.Conv1d(in_channels=1, out_channels=1, \n",
    "                                kernel_size=128, stride=2, padding=0, dilation=1, bias=True)\n",
    "        self.conv1b = nn.Conv1d(in_channels=1, out_channels=1, \n",
    "                                kernel_size=128, stride=2, padding=0, dilation=1, bias=True)\n",
    "        self.pool1d = nn.MaxPool1d(kernel_size=4)\n",
    "        self.batch_embed = nn.BatchNorm1d(1)\n",
    "        self.batch1a = nn.BatchNorm1d(1)\n",
    "        self.batch1b = nn.BatchNorm1d(1)\n",
    "        \n",
    "#         # I would probably get rid of these:\n",
    "#         self.conv2a = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=8, stride=1, padding=1, dilation=1)\n",
    "#         self.conv2b = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=8, stride=1, padding=1, dilation=1)\n",
    "#         self.pool2d = nn.MaxPool2d(2,2)\n",
    "#         self.batch2a = nn.BatchNorm2d(1)\n",
    "#         self.batch2b = nn.BatchNorm2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(241, 64)\n",
    "        self.fc2 = nn.Linear(64, dataset.n_labels)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        for layer in [self.conv1a,self.conv1b,self.fc1,self.fc2]:\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.embed(x))\n",
    "#         x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.batch_embed(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.conv1a(x))\n",
    "        x = self.batch1a(x)\n",
    "        x = self.pool1d(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.conv1b(x))\n",
    "        x = self.batch1b(x)\n",
    "        \n",
    "#         x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
    "#         x = self.drop(x)\n",
    "#         x = F.relu(self.conv2a(x))\n",
    "#         x = self.batch2a(x)\n",
    "#         x = self.pool2d(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = F.relu(self.conv2b(x))\n",
    "#         x = self.batch2b(x)\n",
    "        \n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, batch_size, optimizer, learning_rate, criterion, epochs, \n",
    "          test_pct=0.05, loss_print_freq=50, eval_freq=100):\n",
    "    logging.info('Initialising')\n",
    "    logging.info('Setup')\n",
    "\n",
    "    test_num = math.floor(dataset.n_cells * test_pct)\n",
    "    train_num = dataset.n_cells - test_num\n",
    "\n",
    "    logging.info('Train examples: ' + str(train_num))\n",
    "    logging.info('Test examples: ' + str(test_num))\n",
    "\n",
    "    # Load and split dataset\n",
    "    logging.info('Loading and splitting dataset')\n",
    "    train_set, test_set = torch.utils.data.random_split(dataset, [train_num, test_num])\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "    # ## DEBUG ##\n",
    "    # train_features, train_labels = next(iter(train_dataloader))\n",
    "    # print(f\"Feature batch shape: {train_features.size()}\")\n",
    "    # print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "    # Train model\n",
    "    logging.info('Training model')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % loss_print_freq == loss_print_freq - 1:\n",
    "                single_loss = running_loss / loss_print_freq\n",
    "                train_losses.append(single_loss)\n",
    "                logging.info('[epoch-%d, %5d] loss: %.3f' % (epoch + 1, i + 1, single_loss))\n",
    "                running_loss = 0.0\n",
    "\n",
    "            # eval\n",
    "            if i % eval_freq == eval_freq - 1:\n",
    "                model.eval()\n",
    "                test_loss = 0.0\n",
    "                accuracy = 0.0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_dataloader:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        ps = torch.exp(outputs)\n",
    "                        top_p, top_class = ps.topk(1, dim=1)\n",
    "                        equals = top_class == labels.view(*top_class.shape)\n",
    "                        test_loss += loss.item()\n",
    "                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "                test_count = len(test_dataloader)\n",
    "\n",
    "                print(accuracy)\n",
    "                print(test_count)\n",
    "\n",
    "                single_test_loss = test_loss / test_count\n",
    "                single_test_accuracy = accuracy / test_count\n",
    "                test_losses.append(single_test_loss)\n",
    "                test_accuracy.append(single_test_accuracy)\n",
    "                model.train()\n",
    "\n",
    "                logging.info('EVAL - [epoch-%d, %5d] test_loss: %.3f test_accuracy: %.3f' % (epoch + 1, i + 1, single_test_loss, single_test_accuracy * 100))\n",
    "\n",
    "    logging.info('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This takes your ATAC data (as a directory location) and RNA data and a user defined function \n",
    "    to transform the RNA in a meaningful way.\n",
    "    \n",
    "    It stores your data, performs your transformation and sets them up in an iterable way for pytorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, ATAC_path, RNA_path, r_func, use_cuda=True, float_size=32):\n",
    "        if float_size == 16:\n",
    "            self.dtype = torch.float16\n",
    "        elif float_size == 32:\n",
    "            self.dtype = torch.float32\n",
    "        elif float_size == 64:\n",
    "            self.dtype = torch.float64\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.device = torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        \n",
    "        self.dataset = sc.read(ATAC_path)\n",
    "        self.X = self.dataset.X.todense()\n",
    "        self.n_cells = self.X.shape[0]\n",
    "        self.n_peaks = self.X.shape[1]\n",
    "        \n",
    "        # get ground truth\n",
    "        self.RNA = sc.read(RNA_path)    \n",
    "        self.Y = r_func(self.RNA)\n",
    "        self.n_labels = int(len(np.unique(self.Y)))\n",
    "        self.r_func = r_func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.obs.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.tensor(self.Y[idx], device=self.device, dtype=torch.long)\n",
    "        data = torch.tensor(self.X[idx], device=self.device, dtype=self.dtype)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    basic feed forward model to test things out...\n",
    "    \"\"\"\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__()\n",
    "        # could just do two 1d convs to a feed forward network, keep simple...\n",
    "        \n",
    "        self.conv1a = nn.Conv1d(in_channels=1, out_channels=1, \n",
    "                                kernel_size=64, stride=32, padding=0, dilation=1, bias=True)\n",
    "        self.conv1b = nn.Conv1d(in_channels=1, out_channels=1, \n",
    "                                kernel_size=64, stride=32, padding=0, dilation=1, bias=True)\n",
    "        self.pool1d = nn.MaxPool1d(kernel_size=8)\n",
    "        self.batch_embed = nn.BatchNorm1d(1)\n",
    "        self.batch1a = nn.BatchNorm1d(1)\n",
    "        self.batch1b = nn.BatchNorm1d(1)\n",
    "        \n",
    "        # I would probably get rid of these:\n",
    "#         self.conv2a = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=8, stride=1, padding=1, dilation=1)\n",
    "#         self.conv2b = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=8, stride=1, padding=1, dilation=1)\n",
    "#         self.pool2d = nn.MaxPool2d(2,2)\n",
    "#         self.batch2a = nn.BatchNorm2d(2)\n",
    "#         self.batch2b = nn.BatchNorm2d(4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(352, 32)\n",
    "        self.fc2 = nn.Linear(32, dataset.n_labels)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        \n",
    "        for layer in [self.conv1a,self.conv1b,self.fc1,self.fc2]:\n",
    "            nn.init.xavier_normal_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.conv1a(x))\n",
    "        x = self.batch1a(x)\n",
    "        x = self.pool1d(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.conv1b(x))\n",
    "        x = self.batch1b(x)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "dataset = Load_Dataset(atac_path, rna_path, celltype_function)\n",
    "model = CNN(dataset)\n",
    "model.to('cuda')\n",
    "########### VARIABLES ##########\n",
    "\n",
    "test_pct = 0.2\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "# momentum = 0.9\n",
    "epochs = 50\n",
    "loss_print_freq = 10\n",
    "eval_freq = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "########### LET'S GO ##########\n",
    "\n",
    "train(dataset, model, batch_size, optimizer, learning_rate, criterion, epochs, \n",
    "      test_pct, loss_print_freq, eval_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_test",
   "language": "python",
   "name": "kernel_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
